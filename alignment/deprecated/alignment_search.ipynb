{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from os import path, listdir\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/media/ula/DATADRIVE1/fos_gfp_tmaze/ctx_landmark/despeckle/\"\n",
    "res_dir_path = dir_path + \"alignment_result/\"\n",
    "path_for_icy = res_dir_path + \"aligned_despeckle/\"\n",
    "\n",
    "ext = \".tif\"\n",
    "search_window = 15;\n",
    "stack_window = 8;\n",
    "substack_size = 5;\n",
    "\n",
    "max_regions_no = 4;\n",
    "filename_template = \"m{}_r{}_{}_spots\"\n",
    "filename_raw_temp = \"m{}_r{}_{}\"\n",
    "resfilename_template = \"m{}r{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_substack_trans(substack1, substack2):\n",
    "    minv = 5000\n",
    "    minx = 0\n",
    "    miny = 0\n",
    "    for x in range(-search_window, search_window):\n",
    "        for y in range(-search_window, search_window):\n",
    "            xdif = min(substack1.shape[1], substack2.shape[1])-abs(x);\n",
    "            ydif = min(substack1.shape[2], substack2.shape[2])-abs(y);\n",
    "            sub1_tmp = substack1[:,max(0,x):max(0,x)+xdif,max(0,y):max(0,y)+ydif]\n",
    "            sub2_tmp = substack2[:,max(0,-x):max(0,-x)+xdif,max(0,-y):max(0,-y)+ydif]\n",
    "            dif_tmp = abs(sub1_tmp - sub2_tmp)\n",
    "            tmp_avg = np.average(dif_tmp)\n",
    "            if tmp_avg < minv:\n",
    "                minv = tmp_avg\n",
    "                minx = x\n",
    "                miny = y\n",
    "    return np.array([[minx],[miny]]), minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stack_translations(stack1, stack2):\n",
    "    translations = np.array([[],[]])\n",
    "    stack_size = stack1.shape[0]\n",
    "    \n",
    "    sum_of_avgs = 0\n",
    "    \n",
    "    start_idx = 0\n",
    "    while start_idx + substack_size < stack_size:\n",
    "        stop_idx = start_idx + substack_size\n",
    "        if stop_idx + substack_size > stack_size:\n",
    "            stop_idx = stack_size+1\n",
    "        coords, sub_avg = find_optimal_substack_trans(stack1[start_idx:stop_idx], stack2[start_idx:stop_idx])\n",
    "        sum_of_avgs += sub_avg\n",
    "        translations = np.append(translations, coords, axis = 1)\n",
    "        start_idx = stop_idx\n",
    "    return translations, sum_of_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alignment_translations(mouse, region, starting_session_id, file_suffix, session_order):\n",
    "    sn = session_order[starting_session_id-1:starting_session_id+1]\n",
    "    fn0 = None\n",
    "    legacy = None\n",
    "    raw_legacy = None\n",
    "    \n",
    "    if starting_session_id == 1:\n",
    "        fn1 = dir_path+filename_template.format(mouse, region,sn[0]) +ext\n",
    "        raw1 = dir_path+filename_raw_temp.format(mouse, region,sn[0]) +ext\n",
    "    elif starting_session_id == 2:\n",
    "        raw_legacy = path_for_icy+filename_raw_temp.format(mouse, region,session_order[0]) + ext\n",
    "        fn0 = res_dir_path+filename_template.format(mouse, region,session_order[0]) +ext\n",
    "        fn1 = res_dir_path+filename_template.format(mouse, region,sn[0]) +ext\n",
    "        raw1 = path_for_icy+filename_raw_temp.format(mouse, region,sn[0]) +ext\n",
    "    fn2 = dir_path+filename_template.format(mouse, region,sn[1]) +ext\n",
    "    raw2 = dir_path+filename_raw_temp.format(mouse, region,sn[1]) +ext\n",
    "    print(fn1)\n",
    "    if path.exists(fn1) and path.exists(fn2):\n",
    "        orig1 = io.imread(fn1).astype(\"uint8\")\n",
    "        orig2 = io.imread(fn2).astype(\"uint8\")\n",
    "        raw1 = io.imread(raw1)\n",
    "        raw2 = io.imread(raw2)\n",
    "        \n",
    "        \n",
    "        if starting_session_id == 2:\n",
    "            legacy = io.imread(fn0).astype(\"uint8\") \n",
    "            raw_legacy = io.imread(raw_legacy).astype(\"uint8\") \n",
    "            \n",
    "        \n",
    "        z1 = orig1.shape[0]\n",
    "        z2 = orig2.shape[0]\n",
    "        optimal_sum_of_avgs = 50000\n",
    "        optimal_translations = []\n",
    "        minz = 0\n",
    "        for z in range(-stack_window, stack_window):\n",
    "            tst1 = orig1[max(0,z) : min(z1, z2+z)];\n",
    "            tst2 = orig2[max(0,-z) : min(z1-z, z2)];\n",
    "\n",
    "            coords_list, sum_of_avgs = find_stack_translations(tst1, tst2)\n",
    "            if sum_of_avgs < optimal_sum_of_avgs:\n",
    "                optimal_sum_of_avgs = sum_of_avgs\n",
    "                optimal_translations = coords_list\n",
    "                minz = z\n",
    "        aligned1, aligned2, legacy_ = align_stacks(orig1, orig2, optimal_translations, \n",
    "                                                   minz, starting_session_id, legacy_stack = legacy)\n",
    "        save_results(aligned1, aligned2, optimal_translations, minz, \n",
    "                     starting_session_id, mouse, region, res_dir_path, \n",
    "                     filename_template, file_suffix, session_order)\n",
    "        \n",
    "        save_single_img(res_dir_path+filename_template.format(mouse, region,session_order[0])+ext,\n",
    "                        legacy_)\n",
    "        \n",
    "\n",
    "\n",
    "        aligned1_raw, aligned2_raw, raw_legacy_ = align_stacks(raw1, raw2,optimal_translations, minz, \n",
    "                                                     starting_session_id, legacy_stack = raw_legacy)\n",
    "\n",
    "        save_single_img(path_for_icy+filename_raw_temp.format(mouse, region,session_order[0])+ext,\n",
    "                        raw_legacy_)\n",
    "        save_results(aligned1_raw, aligned2_raw, optimal_translations, minz, starting_session_id,\n",
    "                     mouse, region, path_for_icy,filename_raw_temp, file_suffix,session_order, \n",
    "                     to_csv = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_adj_translation(max_tr, curr_trans, direction):\n",
    "    #direction - jesli sesja subsequent to -1\n",
    "    #do sprawdzenia nawiasy cuda wianki\n",
    "    return int(max(0, (max(0, max_tr * direction) + max_tr-curr_trans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dims_post_alignment(orig_dim1, orig_dim2, translation_arr):\n",
    "    max_trans = int(max(translation_arr, key = abs))\n",
    "    if (np.sign(min(translation_arr)) * np.sign(max(translation_arr)) == -1):\n",
    "        return max_trans, int(min(orig_dim1, orig_dim2) - (max(translation_arr) - min(translation_arr)))\n",
    "    return max_trans, min(orig_dim1, orig_dim2) - abs(max_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_stacks(orig1, orig2, optimal_translations, minz, start_img_id, legacy_stack = None):\n",
    "    z1 = orig1.shape[0]\n",
    "    z2 = orig2.shape[0]\n",
    "    res1 = orig1[max(0,minz) : min(z1, z2+minz)];\n",
    "    res2 = orig2[max(0,-minz) : min(z1-minz, z2)];\n",
    "\n",
    "    print(\"optimal trans \", optimal_translations)\n",
    "    print(\"minz \", minz)\n",
    "\n",
    "\n",
    "    max_transx, difx = find_dims_post_alignment(res1.shape[1], res2.shape[1], optimal_translations[0])\n",
    "    max_transy, dify = find_dims_post_alignment(res1.shape[2], res2.shape[2], optimal_translations[1])\n",
    "\n",
    "    ret1 = np.empty((res1.shape[0], difx, dify))\n",
    "    ret2 = np.empty_like(ret1)\n",
    "    \n",
    "    legacy_ret = None\n",
    "    if legacy_stack is not None:\n",
    "        legacy_stack = legacy_stack[max(0,minz) : min(z1, z2+minz)];\n",
    "        legacy_ret = np.empty_like(ret1)\n",
    "        \n",
    "    x0 = [calc_adj_translation(max_transx,x, -1) for x in optimal_translations[0]]\n",
    "    y0 = [calc_adj_translation(max_transy,y, -1) for y in optimal_translations[1]]\n",
    "    \n",
    "    x0_b = max(0, max_transx)\n",
    "    y0_b = max(0, max_transy)\n",
    "    for idx, (r1, r2) in enumerate(zip(res1,res2)):\n",
    "        tr_idx = idx//substack_size\n",
    "        if tr_idx >= len(x0):\n",
    "            tr_idx = len(x0) - 1\n",
    "        ret1[idx] =  r1[x0_b:x0_b+difx,y0_b:y0_b+dify]\n",
    "        if legacy_stack is not None:\n",
    "            legacy_ret[idx] = legacy_stack[idx][x0_b:x0_b+difx,y0_b:y0_b+dify]\n",
    "        ret2[idx] =  r2[x0[tr_idx]:x0[tr_idx]+difx,y0[tr_idx]:y0[tr_idx]+dify]\n",
    "    return ret1, ret2, legacy_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_tif_save(image):\n",
    "    newimg = np.empty((image.shape[0], 1, image.shape[1], image.shape[2]))\n",
    "    for idx, im in enumerate(image):\n",
    "        newimg[idx] = np.array([im])\n",
    "    return newimg.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_single_img(img_path, image):\n",
    "    if image is not None:\n",
    "        image = prepare_for_tif_save(image)\n",
    "        tifffile.imwrite(img_path, image, imagej=True, metadata={'unit': 'pixels','axes': 'ZCYX'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(ret1, ret2, optimal_translations, minz, start_img_id, mouse, region, \n",
    "                 res_dir,fname_template, file_suffix,session_order,  to_csv=True):\n",
    "    ret1 = prepare_for_tif_save(ret1)\n",
    "    ret2 = prepare_for_tif_save(ret2)\n",
    "    findif = abs(ret1-ret2)\n",
    "    \n",
    "    sn = session_order[start_img_id-1:start_img_id+1]\n",
    "    \n",
    "    tifffile.imwrite(res_dir+fname_template.format(mouse, region,sn[0])+ext,ret1.astype('uint8'),imagej=True, metadata={'unit': 'pixels','axes': 'ZCYX'})\n",
    "    tifffile.imwrite(res_dir+fname_template.format(mouse, region,sn[1])+ext, ret2.astype('uint8'),imagej=True, metadata={'unit': 'pixels','axes': 'ZCYX'})\n",
    "    tifffile.imwrite(res_dir+\"m\"+str(mouse)+\"r\"+str(region)+\"_\"+file_suffix+ext,findif.astype('uint8'),imagej=True, metadata={'unit': 'pixels','axes': 'ZCYX'})\n",
    "    \n",
    "    if to_csv:\n",
    "        with open(res_dir+\"m\"+str(mouse)+\"r\"+str(region)+\"_\"+str(start_img_id)+file_suffix+ '.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(optimal_translations[0])\n",
    "            writer.writerow(optimal_translations[1])\n",
    "            writer.writerow(str(minz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_first_mice = [1, 3, 4, 6,7,9,12,15,17, 18]\n",
    "landmark_first_mice = [4, 15]\n",
    "ctx_first_mice = [2,5,8,10,11,13,14,16,19,20]\n",
    "ctx_first_mice = [13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_all_sessions(m,r, session_order_code):\n",
    "    session_order = sessions[session_order_code]\n",
    "    print(m, r, session_order)\n",
    "    for start_img_id in [1,2]:\n",
    "        suff = \"_\" + session_order[start_img_id-1] + \"_\" + session_order[start_img_id]\n",
    "        find_alignment_translations(m, r, start_img_id, suff, session_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions={\n",
    "'l' : [\"landmark\", \"ctx1\", \"ctx2\"],\n",
    "'c' : [\"ctx\", \"landmark1\", \"landmark2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 3 ['ctx', 'landmark1', 'landmark2']\n",
      "/media/ula/DATADRIVE1/fos_gfp_tmaze/ctx_landmark/despeckle/m13_r3_ctx_spots.tif\n",
      "optimal trans  [[-2. -1.  0.  2.  3.  3.  4.  4.]\n",
      " [ 2.  2.  1.  1.  1.  1.  1.  0.]]\n",
      "minz  1\n",
      "optimal trans  [[-2. -1.  0.  2.  3.  3.  4.  4.]\n",
      " [ 2.  2.  1.  1.  1.  1.  1.  0.]]\n",
      "minz  1\n",
      "/media/ula/DATADRIVE1/fos_gfp_tmaze/ctx_landmark/despeckle/alignment_result/m13_r3_landmark1_spots.tif\n",
      "optimal trans  [[-4. -4. -3. -1.  0.  0.  1.  0.]\n",
      " [-2. -2. -2. -2. -1. -1. -2. -3.]]\n",
      "minz  -2\n",
      "optimal trans  [[-4. -4. -3. -1.  0.  0.  1.  0.]\n",
      " [-2. -2. -2. -2. -1. -1. -2. -3.]]\n",
      "minz  -2\n"
     ]
    }
   ],
   "source": [
    "processing_queue = [ (13,3,'c')]\n",
    "#(2,2,'c'), (5,3,'c'), (8,2,'c'), (10,3,'c'), (11,3,'c'), (12,1,'l'), (12,2,'l'), (12,3,'l')]#,]\n",
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool(8) as p:\n",
    "    p.starmap(align_all_sessions, processing_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
